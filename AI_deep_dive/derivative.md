---
id: 1738159355-DIUJ
aliases:
  - derivative
tags:
  - derivative
---

# derivative
## Q. 도함수가 뭐야?
도함수(derivative)는 어떤 함수의 **변화율**을 나타내는 함수야. 쉽게 말하면, **함수가 변하는 속도를 측정하는 도구**라고 생각하면 돼.

---

## 1. **도함수란?**
어떤 함수 $f(x)$가 있을 때, 도함수 $f'(x)$는 그 함수가 특정한 점에서 얼마나 빠르게 변하는지를 나타내. 

예를 들어, $y = f(x)$ 그래프에서 한 점을 골라서 그 점에서 **기울기**를 구한다고 생각하면 돼.

- **함수의 기울기(=변화율)**: $f(x)$가 $x$에 따라 얼마나 변하는가?
- **도함수란?** $x$에 대해 $f(x)$의 기울기를 구하는 것.

도함수는 수학적으로 다음과 같이 정의돼:

$$
f'(x) = \lim_{{h \to 0}} \frac{f(x+h) - f(x)}{h}
$$

이건 **한 점에서의 순간 변화율(기울기)** 을 구하는 식이야.

---

## 2. **도함수의 예시**
함수를 몇 개 들면서 보면 이해가 쉬울 거야.

### (1) **1차 함수: $f(x) = 3x + 2$**
이 함수는 직선이야. 기울기가 일정하니까 변화율도 일정해.

$$
f'(x) = 3
$$

즉, 이 함수는 어디에서든 기울기가 **3**으로 변하지 않아.

---

### (2) **2차 함수: $f(x) = x^2$**
이제 곡선이 나오면 이야기가 달라져.

$$
f'(x) = 2x
$$

- $x = 1$일 때: $f'(1) = 2(1) = 2$ → 이 점에서 기울기는 2
- $x = 3$일 때: $f'(3) = 2(3) = 6$ → 기울기가 더 가팔라짐
- $x = -2$일 때: $f'(-2) = 2(-2) = -4$ → 음수니까 이 부분에선 그래프가 내려감

즉, $f(x) = x^2$의 기울기는 $x$ 값에 따라 계속 변해.

---

## 3. **비선형 활성화 함수의 도함수**
활성화 함수들이 왜 비선형인지 설명할 때 도함수를 봤었는데, 다시 한 번 살펴보면:

### (1) **ReLU: $f(x) = \max(0, x)$**
$$
f'(x) =
\begin{cases} 
1, & x > 0 \\ 
0, & x \leq 0 
\end{cases}
$$
→ 0 이하에서는 기울기가 0이라 학습이 안 되고, 양수에서는 기울기가 1로 유지돼.

### (2) **Sigmoid: $f(x) = \frac{1}{1 + e^{-x}}$**
$$
f'(x) = f(x)(1 - f(x))
$$

![sigmoid.png](assets/imgs/sigmoid.png)

→ 도함수가 항상 0과 0.25 사이에 있어서 학습 속도가 느려질 수 있어.

### (3) **Tanh: $f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$**
$$
f'(x) = 1 - f(x)^2
$$
→ 도함수 값이 -1과 1 사이에서 변화해.

---

## 4. **도함수가 중요한 이유**
- **신경망에서 학습(Gradient Descent)할 때 기울기를 사용해서 가중치를 조정**해야 해.
- **도함수가 0에 가까우면 학습이 거의 안 됨** → "기울기 소실([[1738159475-TWPO|Vanishing Gradient]])" 문제 발생.
- **ReLU는 0보다 크면 기울기가 1로 유지**돼서 깊은 신경망에서도 학습이 잘 됨.

즉, 도함수는 **"이 함수가 얼마나 빠르게 변하고 있는가?"** 를 측정하는 도구이고, 특히 머신러닝에서는 학습 속도와 관련이 깊어!
