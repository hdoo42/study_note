---
id: 1738162050-PHWD
aliases:
  - Explanation with examples about why linear transformation is not enough
tags: []
---

# Explanation with examples about why linear transformation is not enough
### **비선형 활성화 함수의 필요성: ReLU의 역할**

#### **1. 선형 변환만으로는 심층 학습이 불가능한 이유**
신경망은 기본적으로 여러 층(layer)으로 구성된 연산 구조이며, 각 층에서는 다음과 같은 선형 변환이 적용된다.

$$
\mathbf{y} = \mathbf{W} \mathbf{x} + \mathbf{b}
$$

여기서:
- $\mathbf{x}$는 입력 벡터
- $\mathbf{W}$는 가중치 행렬
- $\mathbf{b}$는 편향(bias) 벡터
- $\mathbf{y}$는 출력 벡터

만약 신경망에서 모든 층이 선형 변환(즉, 행렬 곱셈과 덧셈)만 수행한다고 가정하자. 그러면, 두 개의 층을 차례로 적용하는 것은 다음과 같이 표현된다.

$$
\mathbf{y}_2 = \mathbf{W}_2 (\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1) + \mathbf{b}_2
$$

이를 전개하면:

$$
\mathbf{y}_2 = (\mathbf{W}_2 \mathbf{W}_1) \mathbf{x} + (\mathbf{W}_2 \mathbf{b}_1 + \mathbf{b}_2)
$$

즉, 단순히 하나의 선형 변환과 동일한 형태로 축약된다.  
아무리 많은 층을 추가하더라도 결국 하나의 행렬 변환 $\mathbf{W}_{\text{total}} \mathbf{x} + \mathbf{b}_{\text{total}}$로 표현될 수 있으며, 신경망의 학습 능력이 제한된다. **이것이 선형 신경망의 한계이다.**

---

#### **2. ReLU를 포함한 비선형 활성화 함수의 역할**
비선형 활성화 함수 $f(x)$를 적용하면, 선형 변환과 달리 **모든 층을 합쳐도 단일 행렬 연산으로 축약되지 않는다.**  
예를 들어, 두 층을 가진 신경망에서 활성화 함수 $f(x)$ (예: ReLU)를 포함하면:

$$
\mathbf{y}_1 = f(\mathbf{W}_1 \mathbf{x} + \mathbf{b}_1)
$$

$$
\mathbf{y}_2 = f(\mathbf{W}_2 \mathbf{y}_1 + \mathbf{b}_2)
$$

이때, 활성화 함수 $f(x)$가 비선형 함수이므로, 전체 표현은 단일 행렬 변환으로 단순화되지 않는다.

##### **예제: 선형 변환만 있을 때와 ReLU를 사용할 때 비교**
##### **Case 1: 선형 변환만 사용**
입력 $x$를 거치는 두 개의 층이 다음과 같다고 가정하자.

$$
y_1 = 2x + 3
$$
$$
y_2 = 5y_1 + 1
$$

이를 직접 계산하면:

$$
y_2 = 5(2x + 3) + 1 = 10x + 15 + 1 = 10x + 16
$$

즉, 두 개의 층이 단일 선형 변환 $10x + 16$으로 축약되므로, 깊은 신경망을 만들 의미가 없다.

##### **Case 2: ReLU 활성화 함수 적용**
이제 ReLU를 추가하여:

$$
y_1 = \max(0, 2x + 3)
$$
$$
y_2 = \max(0, 5y_1 + 1)
$$

입력 $x$가 -5라고 가정하면:

1. $y_1 = \max(0, 2(-5) + 3) = \max(0, -10 + 3) = \max(0, -7) = 0$
2. $y_2 = \max(0, 5(0) + 1) = \max(0, 1) = 1$

입력 $x$가 3이라고 가정하면:

1. $y_1 = \max(0, 2(3) + 3) = \max(0, 6 + 3) = \max(0, 9) = 9$
2. $y_2 = \max(0, 5(9) + 1) = \max(0, 45 + 1) = 46$

이처럼 ReLU를 사용하면 층을 거칠 때마다 새로운 비선형성을 부여하여, 단순한 선형 관계로 축약되지 않고 복잡한 표현력을 가질 수 있다.

---

#### **3. 결론**
- 선형 변환만 사용하면 신경망이 하나의 선형 변환으로 축약되어 깊은 학습이 불가능하다.
- ReLU와 같은 비선형 활성화 함수를 사용하면 네트워크의 표현력이 증가하여 깊은 학습이 가능해진다.
- 특히 ReLU는 **희소성(sparsity)과 기울기 소실 문제(vanishing gradient problem) 해결** 등의 장점을 제공하여 심층 신경망에서 널리 사용된다.
