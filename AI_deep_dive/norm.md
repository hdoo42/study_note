---
id: norm
aliases: []
tags: []
---

# norm
아래 설명은 **노름(Norm)** 의 일반적인 개념과, 그중 **L2 노름**(유클리드 노름) 및 머신러닝에서 자주 언급되는 **L2 정규화(Weight Decay)** 와의 관련성을 다룬다.

---

## 1. 노름(Norm)이란?

**노름(Norm)** 은 수학, 특히 선형대수학과 벡터 해석학 등에서 **벡터(또는 행렬 등 수학적 객체)의 ‘길이’ 혹은 ‘크기’를 측정하는 함수**이다.  
일반적으로, $\mathbf{x}$가 $\mathbb{R}^n$에 속하는 벡터라 할 때, 노름은 다음과 같은 조건을 만족하는 함수 $\|\cdot\|$를 말한다.

1. **양의 정의(Positivity)**:  
   $\|\mathbf{x}\| \ge 0$이며, $\|\mathbf{x}\| = 0$ $\Leftrightarrow$ $\mathbf{x} = \mathbf{0}$.
   * [[if_and_only_if|↔]] 에 대하여
2. **균등 확대([[homogeneity|Homogeneity]], 절댓값에 대한 양의 동차성)**:  
   모든 스칼라 $\alpha \in \mathbb{R}$에 대해, $\|\alpha \mathbf{x}\| = |\alpha| \|\mathbf{x}\|$.
3. **삼각 부등식([[triangle_inequality|Triangle Inequality]])**:  
   $\|\mathbf{x} + \mathbf{y}\| \le \|\mathbf{x}\| + \|\mathbf{y}\|$.

### 예시: $p$-노름
- $\|\mathbf{x}\|_p = \left(\sum_{i=1}^n |x_i|^p \right)^{1/p}$
- 이때 $p \ge 1$이면 위 세 가지 조건을 만족하여 노름이 된다.
- 대표적으로 $p = 1$이면 L1 노름, $p = 2$이면 L2 노름, $p = \infty$이면 최대 노름($\max$ 노름)이 된다.

---

## 2. L2 노름(L2 norm)이란?

$$
\|\mathbf{x}\|_2 = \sqrt{\sum_{i=1}^n x_i^2}
$$

- **유클리드 노름(Euclidean norm)**이라고도 부르며, 2차원/3차원 공간에서 익숙한 “유클리드 거리” 개념과 동일하다.  
- 예를 들어, 2차원 벡터 $\mathbf{x} = (x_1, x_2)$의 L2 노름은 $\sqrt{x_1^2 + x_2^2}$로, 이는 $\mathbf{x}$가 원점(0, 0)으로부터 떨어진 거리와 같다.
- 고차원으로 확장하면, $\sqrt{\sum_i x_i^2}$가 “원점으로부터의 거리”로 해석된다.

---

## 3. L2 정규화(Weight Decay)와의 관계

머신러닝(특히 딥러닝)에서 **L2 정규화**는 보통 “Weight Decay(가중치 감쇄)”라는 용어로도 많이 불린다.  
목적은 “가중치가 과도하게 커지는 현상”을 억제해서 **과적합(Overfitting)을 방지**하고자 하는 것이다.  
일반적으로 **손실 함수**(Loss)에 다음 항을 추가한다.

$$
\text{Loss}_{\text{total}} 
= \text{Loss}_{\text{original}} + \lambda \sum_i (w_i^2)
\;\; \Longleftrightarrow \;\;
= \text{Loss}_{\text{original}} + \lambda \|\mathbf{w}\|_2^2
$$

- $\mathbf{w}$는 모델의 파라미터(가중치) 벡터.  
- $\lambda$는 정규화 강도를 결정하는 하이퍼파라미터.

### **효과**
- 가중치가 커질수록 $\sum w_i^2$가 커져 손실이 증가하므로, **가중치를 작게 유지**하는 방향으로 학습이 진행된다.  
- L1 정규화(절댓값 $\sum |w_i|$)와 달리, **가중치가 0이 되기보다는 전반적으로 작아지는 형태**를 유도한다.

---

## 4. L2 노름과 다른 노름과의 비교

| **구분**    | **L2 노름**                                       | **L1 노름**                                        | **L\_\infty 노름**                     |
|:-----------:|:-------------------------------------------------:|:--------------------------------------------------:|:--------------------------------------:|
| **정의**    | $\sqrt{\sum_i x_i^2}$                           | $\sum_i |x_i|$                                   | $\max_i |x_i|$                       |
| **해석**    | 유클리드 거리                                     | 좌표값의 절댓값 합                                 | 최대 절댓값 (Chebyshev 거리)           |
| **특징**    | 부드럽게 0 근처로 수렴, 안정적                     | 일부 좌표(가중치)는 정확히 0이 될 수 있음(희소성) | 가장 큰 좌표가 전체 값 결정            |
| **장점**    | - 미분 가능(0에서 불연속 X)<br>- L2 정규화는 안정적 | - 특성 선택 가능<br>- 가중치가 0으로 수렴 가능       | - 잡음에 예민 <br>- 특이값 감지에 유리 |
| **사용 예** | Ridge 회귀, L2 정규화                             | Lasso 회귀, L1 정규화                             | 일부 특수한 최적화/로버스트 분석 등    |

---

## 5. 요약

1. **노름(Norm)**: 벡터(혹은 다양한 수학적 대상)의 크기(길이)를 정의하는 함수로, 수학에서 매우 중요한 개념이다.  
2. **L2 노름**: 가장 흔히 쓰이는 노름으로, 유클리드 거리의 일반화된 형태. $\sqrt{\sum_i x_i^2}$로 계산한다.  
3. **L2 정규화**: 모델의 가중치 벡터에 L2 노름(또는 그 제곱)을 페널티로 부여해, 파라미터가 지나치게 커지지 않도록 제어하는 방식. **과적합 방지**와 **학습 안정성**을 도모한다.  

따라서, “L2”는 **벡터의 크기를 잴 때 2제곱을 사용하는 방식**(즉, 유클리드 노름)을 의미하며, “노름”은 이와 같이 **벡터의 크기를 정의하는 여러 방법**(L1, L2, L\_\infty 등)을 통칭한다고 볼 수 있다.
