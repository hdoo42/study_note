---
id: xor_nn_example
aliases:
  - XOR implement with neural network
tags:
  - dl
---

# XOR implement with neural network

### **실제 예제: XOR 문제를 해결하는 신경망 (은닉 뉴런 포함)**

간단한 **XOR(배타적 논리합) 문제**를 해결하는 신경망을 예제로 들면, **은닉 뉴런의 역할을 직관적으로 이해**할 수 있습니다.

---

## **1. XOR 문제란?**
XOR(Exclusive OR)은 아래처럼 작동하는 논리 연산입니다.

| $x_1$ | $x_2$ | $y = x_1 \oplus x_2$ |
|---------|---------|----------------|
| 0       | 0       | 0              |
| 0       | 1       | 1              |
| 1       | 0       | 1              |
| 1       | 1       | 0              |

- **XOR의 특징**: 선형 분리가 불가능함 → 선형 모델(퍼셉트론 1개)로 해결할 수 없음.
- **해결법**: **은닉 뉴런을 추가하여 다층 신경망([[MLP 1|MLP 1]], Multi-Layer Perceptron)을 구성**하면 해결 가능!

---

## **2. XOR을 해결하는 신경망 구조 (은닉 뉴런 2개 사용)**

이제, **은닉 뉴런 2개를 포함한 신경망을 만들어 XOR 문제를 해결**해보겠습니다.

### **네트워크 구조**
1. **입력층 (Input Layer):** $x_1, x_2$ (입력값)
2. **은닉층 (Hidden Layer):** 2개의 은닉 뉴런 $h_1, h_2$
3. **출력층 (Output Layer):** 1개의 출력 뉴런 $y$

---

### **3. 수식으로 표현하기**
각 은닉 뉴런의 출력을 계산하는 수식을 정의해 보겠습니다.

#### **1) 은닉층 (Hidden Layer)**
은닉 뉴런 $h_1, h_2$의 출력:

$$
h_1 = \sigma(w_{11} x_1 + w_{12} x_2 + b_1)
$$

$$
h_2 = \sigma(w_{21} x_1 + w_{22} x_2 + b_2)
$$

(여기서 $\sigma$는 시그모이드([[sigmoid 1|sigmoid 1]]) 함수 또는 [[ReLU 1|ReLU 1]] 같은 활성화 함수)

#### **2) 출력층 (Output Layer)**
출력 뉴런의 출력:

$$
y = \sigma(w_{o1} h_1 + w_{o2} h_2 + b_o)
$$

여기서:
- $w_{o1}$의 **$_o$** 는 "출력층(Output layer)"을 의미
- 숫자 **$_1$** 은 첫 번째 은닉 뉴런(h₁)을 가리킴  
→ 즉, $w_{o1}$은 "은닉층의 첫 번째 뉴런($h_1$) → 출력층($y$)으로 가는 연결의 가중치"

이 구조를 사용하면 XOR을 해결할 수 있습니다!

---

### **4. 실제 숫자로 예제 계산**
#### **(1) 가중치 및 편향 설정**
적절한 가중치와 편향을 선택하면 XOR을 학습 없이 해결할 수 있습니다.

$$
w_{11} = 1, \quad w_{12} = 1, \quad b_1 = -0.5
$$

$$
w_{21} = -1, \quad w_{22} = -1, \quad b_2 = 1.5
$$

$$
w_{o1} = 1, \quad w_{o2} = 1, \quad b_o = -0.5
$$

활성화 함수는 [[sigmoid 1|**시그모이드(𝜎)**]] 를 사용한다고 가정.

$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

---

#### **(2) XOR 입력에 대한 계산 예시**
예제: $(x_1, x_2) = (1, 0)$ 입력

1. **은닉 뉴런 계산**
   - $h_1 = \sigma(1 \cdot 1 + 1 \cdot 0 - 0.5) = \sigma(0.5) \approx 0.622$
   - $h_2 = \sigma(-1 \cdot 1 + (-1) \cdot 0 + 1.5) = \sigma(0.5) \approx 0.622$

2. **출력 뉴런 계산**
   - $y = \sigma(1 \cdot 0.622 + 1 \cdot 0.622 - 0.5) = \sigma(0.744) \approx 0.678$

출력 $y$가 0.5보다 크므로, **출력은 1**로 해석됩니다.  
(XOR의 정답과 일치!)

---

## **5. 결론: 은닉 뉴런이 없으면 XOR을 풀 수 없다!**
XOR 문제는 **선형 모델로 해결할 수 없지만**, **은닉 뉴런 2개만 추가하면 해결 가능**합니다.

✔ **"은닉 뉴런이 하는 역할"**  
- 단순 선형 변환만으로는 불가능한 **비선형 변환(Nonlinear Transformation)**을 수행.
- 입력 데이터를 **새로운 특징 공간(Feature Space)**으로 변형하여, 최종 출력이 선형적으로 분리되도록 함.

✔ **즉, 은닉 뉴런이 XOR을 풀 수 있도록 중요한 특징을 학습한다!**  
(이 원리가 확장되면, CNN, Transformer 같은 복잡한 신경망에서도 중요한 역할을 하게 됨.)

---

## **6. Python 코드 예제 (실제로 실행해볼 수 있음)**
```python
import numpy as np

# 시그모이드 함수 정의
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# XOR을 해결하는 신경망 (가중치 및 편향 하드코딩)
def xor_nn(x1, x2):
    # 은닉층 뉴런 2개
    h1 = sigmoid(1*x1 + 1*x2 - 0.5)
    h2 = sigmoid(-1*x1 + (-1)*x2 + 1.5)

    # 출력 뉴런
    y = sigmoid(1*h1 + 1*h2 - 0.5)
    
    return y

# XOR 문제 입력값 테스트
inputs = [(0,0), (0,1), (1,0), (1,1)]
for x1, x2 in inputs:
    output = xor_nn(x1, x2)
    print(f"XOR({x1}, {x2}) = {output:.4f}")
```

✔ **실제로 실행하면 0, 1에 가까운 값이 나와 XOR을 해결하는 것을 확인할 수 있음!**

---

## **7. 요약**
✅ **은닉 뉴런(Hidden Neuron)은 단순한 선형 모델이 풀 수 없는 문제를 해결하는 핵심 요소**  
✅ **XOR 문제처럼 선형 분리 불가능한 데이터도, 은닉 뉴런을 추가하면 해결 가능**  
✅ **은닉 뉴런은 입력 데이터를 새로운 특징 공간으로 변환하여 신경망이 학습할 수 있도록 돕는 역할**  
✅ **Python 코드로 직접 XOR을 해결하는 신경망을 구현해볼 수 있음**  
