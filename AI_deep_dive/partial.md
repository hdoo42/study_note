---
id: partial
aliases:
  - Partial
tags:
  - math
  - calculus
---

# What is Partial?
### **∂ (partial, 편미분)이란?**
**partial(편미분, ∂)** 은 **여러 개의 변수를 가진 함수**에서 **특정한 하나의 변수에 대해서만 미분**하는 것을 의미해. 

즉, **다변수 함수(변수가 여러 개인 함수)** 에서 한 변수에 대한 변화율을 구하는 것이라고 생각하면 돼.

---

## **1. 편미분이 필요한 이유**
보통 머신러닝에서 다루는 **손실 함수 $L$** 같은 함수들은 **여러 개의 변수(가중치, 입력값 등)** 를 포함해. 예를 들어, 손실 함수 $L$은 다음처럼 가중치 $w$와 입력값 $x$에 의존할 수 있어:

$$
L(w, x) = (wx - y)^2
$$

이 경우, $L$을 미분하려고 할 때,
- **$w$에 대한 변화율**만 알고 싶을 수도 있고,
- **$x$에 대한 변화율**을 알고 싶을 수도 있어.

이때 **"특정한 하나의 변수에 대해 미분한다"** 는 개념이 필요하고, 그게 **편미분(Partial Derivative, ∂)** 이야.

---

## **2. 편미분의 표기법**
$$
\frac{\partial L}{\partial w}
$$

이걸 **"L을 w에 대해 편미분한다"** 라고 읽어.

- $\partial L$ → 손실 함수 $L$의 변화량
- $\partial w$ → 가중치 $w$의 변화량

즉, **"L이 $w$에 대해 얼마나 변화하는지"** 를 의미해.

비교하자면:
- **일반적인 미분(∂ 대신 d 사용)**
  $$
  \frac{d y}{d x}
  $$
  → 함수가 **변수 하나**만 가질 때 사용.
- **편미분(Partial Derivative, ∂ 사용)**
  $$
  \frac{\partial L}{\partial w}
  $$
  → 함수가 **여러 개의 변수**를 가질 때 특정 변수만 미분할 때 사용.

---

## **3. 예제: $L(w, x) = w x^2$**
만약 손실 함수가 다음과 같다고 하자.

$$
L(w, x) = w x^2
$$

이제 **각 변수에 대해 편미분해 보자.**

### ✅ **$w$에 대한 편미분**
$$
\frac{\partial L}{\partial w} = x^2
$$
- **여기서 $x$는 상수처럼 취급**하고 $w$에 대해 미분했어.

### ✅ **$x$에 대한 편미분**
$$
\frac{\partial L}{\partial x} = 2wx
$$
- **이번엔 $w$를 상수처럼 취급**하고 $x$에 대해 미분했어.

---

## **4. 신경망에서 편미분이 어떻게 사용될까?**
신경망에서 **오차 역전파(Backpropagation)** 과정에서는 모든 가중치 $w$에 대해 **손실 함수 $L$의 기울기(편미분)** 를 구해야 해.

예를 들어, $L$이 가중치 $w_1, w_2, w_3$에 따라 변한다고 하면:

$$
\frac{\partial L}{\partial w_1}, \quad \frac{\partial L}{\partial w_2}, \quad \frac{\partial L}{\partial w_3}
$$

이렇게 각각 편미분을 구해서 **기울기(Gradient)를 계산**하고, 이를 이용해 **경사 하강법(Gradient Descent)** 으로 가중치를 업데이트하는 거야.

---

## **5. 결론**
- **편미분(∂)은 여러 개의 변수를 가진 함수에서 특정 변수에 대해서만 미분하는 것**.
- **신경망에서는 손실 함수 $L$을 가중치 $w$에 대해 편미분해서 기울기를 구함**.
- **편미분을 사용하면 신경망의 각 가중치가 학습 과정에서 어떻게 변화해야 하는지 알 수 있음**.

즉, **편미분은 딥러닝에서 가중치를 업데이트할 때 꼭 필요한 수학 개념**이야!
