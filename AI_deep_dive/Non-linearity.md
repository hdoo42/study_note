---
id: Non-linearity
aliases: []
tags:
  - Math
  - Graph
---

렐루(ReLU), 시그모이드(Sigmoid), 탄젠트H(Tanh) 같은 함수들이 **비선형 활성화 함수**인 이유는, 이 함수들이 입력값과 출력값의 관계에서 **비선형성(Non-linearity)** 을 가지기 때문이야.

### 1. **비선형성이란?**
비선형성이 있다는 건, 수학적으로 표현하면 단순한 1차 함수 $f(x) = ax + b$ 같은 직선이 아니라, **곡선이 포함된 함수**라는 거야. 즉, 함수의 도함수가 입력값에 따라 달라진다면 비선형 함수라고 볼 수 있어.

---

### 2. **각 함수별 비선형성**
#### (1) **ReLU (Rectified Linear Unit)**
$$
\text{ReLU}(x) = \max(0, x)
$$
- 입력값이 **0 이하일 땐 0**, **양수일 땐 그대로 x**를 반환.
- 절단된 부분(0 이하)이 있어서 선형이 아니야.
- 도함수는 두 개의 다른 값($0$ 또는 $1$)을 가지므로 비선형.

#### (2) **Sigmoid**
$$
\text{Sigmoid}(x) = \frac{1}{1 + e^{-x}}
$$

- 입력값을 **0과 1 사이의 곡선 형태로 압축**.
- 미분하면 항상 변화하는 곡선 형태(즉, 선형이 아님).
- 특히 작은 값에서는 거의 0, 큰 값에서는 거의 1로 수렴하는 **S자 형태**라 비선형.

#### (3) **Tanh (Hyperbolic Tangent)**
$$
\text{Tanh}(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}
$$
- 입력값을 **-1과 1 사이의 값으로 변환**.
- 시그모이드랑 비슷하지만 값이 **0을 중심**으로 대칭.
- [[1738159355-DIUJ|derivative]](도함수)도 비선형적으로 변화해서 뉴런이 선형 조합만으로는 표현할 수 없는 패턴을 학습할 수 있게 함.

---

### 3. **비선형 활성화 함수가 필요한 이유**
만약 활성화 함수가 선형이면 ($f(x) = ax + b$), 아무리 층을 깊게 쌓아도 **결국 선형 변환의 반복**이 돼서 복잡한 패턴을 학습할 수 없어. 비선형성을 추가해야만 신경망이 곡선 형태의 복잡한 함수도 학습할 수 있어.

즉, ReLU, Sigmoid, Tanh 같은 비선형 활성화 함수들이 있어야 다층 신경망이 단순한 선형 회귀 이상의 표현력을 가지게 되는 거야.
