---
id: 1738162610-ZGDT
aliases:
  - Sparsity
tags:
  - Sparsity
---

# Sparsity
### **1. Sparsity(희소성)이란 무엇인가?**

일반적으로 "희소성(Sparsity)"은 **많은 요소가 0이거나 0에 가깝고, 실제로 값이 있는 요소(또는 활성화된 뉴런)가 상대적으로 적은 상태**를 의미한다. 예컨대, 어떤 벡터 $\mathbf{x} \in \mathbb{R}^n$가 있을 때, $\mathbf{x}$의 원소 대부분이 0에 가깝거나 정확히 0이라면, 우리는 이 벡터가 **희소하다(sparse)** 고 말한다.

딥러닝에서 희소성은 주로 **활성화값(activation)** 이 0이거나 미미한 값을 가지는 뉴런의 비율이 높은 상태를 의미한다. 가령, ReLU($\max(0, x)$) 활성화를 적용했을 때, 입력값 $x$가 음수이면 해당 뉴런은 0이 되므로 “꺼져(off)” 버리게 된다. 따라서 양수 입력만 활성화되는 ReLU는 희소성을 자연스럽게 유도한다.

---

### **2. ReLU와 희소성**

1. **ReLU 정의**  
   $$
   \mathrm{ReLU}(x) = \max(0, x)
   $$  
   - 입력값이 0 이하이면 출력이 0이 되므로, 이 구간에서 뉴런이 활성화되지 않는다.
   - 결과적으로, ReLU 층을 통과한 출력 벡터(또는 텐서) 내에서 0인 값이 다수 나타날 수 있으며, 이는 희소성으로 이어진다.

2. **희소성이 유발되는 이유**  
   - ReLU가 음수 입력에 대해 0을 출력하기 때문에, 훈련 과정(역전파)에서 가중치가 특정 방향으로 학습되면서 많은 뉴런들이 음수 영역에 머물게 되면, 그 뉴런들은 아예 0으로 “죽어”버린다(Dying ReLU 문제 참조).  
   - 그러나 완전히 죽지 않았더라도, 상대적으로 적은 뉴런만이 양수로 활성화되어 비선형 변환에 참여하기 때문에, 모델의 관점에서는 “필요한 뉴런”만 살아서 작동하는 형태가 될 수 있다.  
   - 이런 현상은 실제로 특정 Task(예: 이미지 분류)에 특화된 뉴런들만 활성화되고, 다른 뉴런들은 거의 활성화되지 않는 결과를 불러온다. 결과적으로 내적 표현(Internal representation)이 더 간결하고 명확해질 수 있다.

---

### **3. 희소성이 주는 장점**

1. **연산 효율성(Computational efficiency) 개선**  
   - 활성화값이 0인 뉴런들은 이후 층에서 곱해지는 값들도 0이 되어, 실제로는 계산할 필요가 없게 된다(하드웨어적으로 최적화가 가능하다는 전제).  
   - 희소 신호는 벡터나 행렬 곱셈 시 연산량을 줄일 수 있는 여지가 있다(물론 소프트웨어/하드웨어 지원 환경에 따라 달라짐).

2. **해석 가능성(Interpretability) 향상**  
   - 많은 뉴런이 0이면, 실제로 “기여하는” 뉴런의 수가 상대적으로 적어서, 어떤 뉴런이 왜 활성화되었는지를 파악하기 더 쉽다.  
   - 예컨대 이미지 처리를 할 때, 특정 패턴(가령 ‘수직선이 강조된 이미지에서만 활성화’)에만 반응하도록 학습된 뉴런이 명확하게 구분되는 경우가 많다.

3. **과적합(Overfitting) 방지 효과**  
   - 모든 뉴런이 항상 활성화되는 것보다, 필요한 뉴런만 활성화되도록 유도하는 것이 학습 면에서 일반화(generalization)에 유리할 수 있다.  
   - 즉, 모델이 입력 데이터에서 추출된 중요한 특징들만을 사용하여 의사결정을 내리고, 불필요한 특징에 대해서는 뉴런이 활성화되지 않아 모델 복잡도가 상대적으로 낮아질 수 있다.

---

### **4. 희소성을 더 강하게 유도하기 위한 기법들**

1. **L1 정규화([[1738163291-NXPU|L1 Regularization]])**  
   - 손실 함수에 L1 노름([[1738164891-GCEZ|norm]])($\|\mathbf{W}\|_1$) 항을 추가하여, 가중치가 0으로 수렴하도록 유도한다.  
   - 가중치 행렬이 희소해지면, 결국 뉴런 출력값도 더욱 희소해질 수 있다.

2. **스파스 코딩(Sparse Coding)**  
   - 입력 데이터를 소수의 기저 벡터(sparse basis)로 표현하도록 하는 방법.  
   - 이미지, 음성 신호 등에서 주요 요소만 추출하고, 나머지는 0에 가깝게 만드는 학습 과정.

3. **Sparsity Regularization(스파스 페널티 추가)**  
   - 활성화값(예: $\mathrm{ReLU}(Wx+b)$)에 대한 스파스 페널티(예: $\ell_1$ 정규화)를 추가하여, 특정 뉴런이 자주 활성화되지 않도록 강제한다.

4. **Dropout**  
   - 뉴런을 랜덤으로 “끄는(off)” 기법으로, 일종의 확률적 희소성(probabilistic sparsity)을 유도할 수 있다.  
   - 엄밀히 말하면 완전히 동일한 “정적” 희소성 개념은 아니지만, 학습 과정 중 일시적으로 비활성화가 일어나면서 모델의 일반화 성능을 향상시키는 효과가 있다.

---

### **5. 실제 예시**

가령, $\mathrm{ReLU}(x)$ 함수를 사용하는 Fully Connected Layer(완전 연결 계층)의 출력 벡터가 $[0, 3.2, 0, 5.1, 0, 0.7, 0, 0]$와 같이 나왔다고 해보자.

- 이 벡터의 길이가 8이라고 하면, 실제 활성화된 뉴런은 3개뿐(3.2, 5.1, 0.7)이고 나머지 5개는 0이다.
- 이처럼 출력 차원의 60%가 0이 되는, 즉 희소성이 높은 상태가 된다.
- 해당 레이어 다음의 연산에서 0인 뉴런 부분은 곱셈에 참여해도 결과값이 0이 되므로, 연산 비용(실제 구현에 따라 최적화할 수 있다)이 줄어드는 효과가 있다.

---

### **정리**

- **희소성(Sparsity)**은 뉴런의 출력(또는 가중치 등)이 0 혹은 매우 작은 값이 되어 “작동하지 않는” 구성 요소가 대부분이 되는 상태를 뜻한다.  
- **ReLU**와 같은 활성화 함수는 음수 영역을 0으로 만들기 때문에 자연스럽게 뉴런의 희소성을 유도한다.  
- 희소성은 **연산 효율성, 해석 가능성, 과적합 방지** 등의 이점을 가져오며, 이를 높이기 위해 **L1 정규화, 스파스 페널티, Dropout** 등의 기법이 활용된다.  
- 딥러닝에서 희소성은 모델이 입력 데이터의 핵심 특징만 “활성화”하여 효율적으로 학습하는 데 기여한다고 볼 수 있다.
