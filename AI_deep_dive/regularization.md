---
id: regularization
aliases:
  - Regularization
tags:
  - ml
  - regularization
---

# Regularization
### **1. L1 정규화(L1 Regularization)란?**

**L1 정규화**는 머신러닝(특히 회귀나 딥러닝 모델)을 학습할 때 **가중치(또는 파라미터)가 불필요하게 커지는 것을 방지**하고, 동시에 **희소성(Sparsity)** 을 유도하기 위해 사용하는 **정규화(Regularization) 기법**이다.  
일반적으로 모델의 **손실 함수**([[Loss_Function|Loss function]])에 다음과 같은 **제약항(penalty term)** 을 추가하여 구현한다.

$$
\text{Loss}_{\text{total}} = \text{Loss}_{\text{original}} + \lambda \sum_i |w_i|
$$

- $\text{Loss}_{\text{original}}$: 기본적인 손실 함수(예: MSE, 크로스 엔트로피 등)  
- $\lambda$: 정규화 강도를 조절하는 하이퍼파라미터(정규화 계수)  
- $w_i$: 모델 파라미터(가중치)  
- $\sum_i |w_i|$: 모델의 모든 가중치 절댓값의 합  

---

### **2. L1 정규화의 효과**

1. **희소성(Sparsity) 유도**  
   - 가중치의 절댓값 합에 비례하는 페널티가 부여되므로, **일부 가중치가 정확히 0**으로 수렴하는 경향이 있다.  
   - 이는 **특정 특성(feature)** 이 모델에 전혀 기여하지 않도록 만들 수 있어, **특성 선택(feature selection)** 과 유사한 효과를 낸다.

2. **과적합([[Overfitting|Overfitting]]) 방지**  
   - 모델이 복잡해지면서 생기는 과적합을 제어하기 위해, 가중치에 대한 벌칙(Penalty)을 줘서 파라미터가 커지지 못하도록 억제한다.  
   - L2 정규화($\sum w_i^2$)와 비교했을 때, L1 정규화는 가중치가 **0이 되게끔** 만들 수 있어 모델의 복잡도를 보다 적극적으로 줄일 수 있다.

- [ ] 3. **해석 가능성([[Interpretability|Interpretability]]) 향상**  
   - 가중치가 0이 된다는 것은 해당 특성(feature)이 예측에 기여하지 않는다는 의미이므로, 모델이 어떤 특성을 사용하는지를 확인하기 더 쉽다.  
   - 특히 고차원 데이터(예: 유전자 데이터, 텍스트 데이터 등)에서 중요한 특성만을 “자동 선택”하는 효과를 얻을 수 있다.

---

### **3. 예시**

#### **3.1 선형 회귀([[linear_regression|Linear Regression]]) 예시**
- 선형 회귀에서 L1 정규화를 적용한 것을 **라소 회귀([[Lasso_Regression|Lasso Regression]])**라고 한다.  
- 모델 식:  
  $$
  \hat{y} = w_1 x_1 + w_2 x_2 + \dots + w_n x_n + b
  $$  
- 손실 함수(예: 평균제곱오차, MSE)에 $\lambda \sum |w_i|$ 항을 추가한다.  
- 훈련 결과, **필요 없는 변수(특성)는 $w_i=0$으로 만들어버리고, 핵심 특성만 살려서** 모델을 간결화한다.

#### **3.2 딥러닝에서의 활용**
- 딥러닝에서는 일반적으로 L2 정규화([[weight_decay|Weight Decay]])가 더 자주 쓰이지만, L1 정규화를 사용하면 **가중치를 희소화**할 수 있다.  
- 예: CNN 레이어의 가중치에 대해 L1 정규화를 적용하면 특정 필터(커널)가 아예 0이 되어 **스파스 필터**만 남을 수도 있다.  
- **희소성**을 더욱 강조하고 싶을 때, 혹은 **특정 신경망 구조에서 메모리 효율성을 높이고 싶을 때** L1 정규화를 도입하기도 한다.

---

### **4. L1 정규화와 L2 정규화 비교**
| **특징**           | **L1 정규화**                                          | **L2 정규화**                                          |
|:-----------------:|:----------------------------------------------------:|:----------------------------------------------------:|
| **정의**           | $\lambda \sum_i \|w_i\|$                                |  $\lambda \sum_i w_i^2$                              |
| **가중치 분포**    | 일부 가중치를 0으로 만듦 (희소성 유도)                   | 가중치가 전반적으로 고르게 작아짐                     |
| **특성 선택**      | 가능(0이 된 변수는 아예 배제)                            | 직접적인 특성 “배제”는 어려움                         |
| **회귀 모델 예시** | 라소 회귀(Lasso)                                      | 릿지 회귀(Ridge)                                     |
| **장점**           | 모델 단순화 및 해석 용이, 과적합 방지 효과 큼             | 안정적인 수렴, 가중치 폭발 억제, 대부분의 딥러닝 모델에 기본 적용 |
| **단점**           | 미분이 $w=0$ 지점에서 불연속 → 옵티마 찾기 어려울 수 있음 | 특성 선택 기능은 떨어짐, 모든 가중치가 0이 되지는 않음            |

---

### **정리**

- **L1 정규화**는 가중치의 절댓값 합에 대한 벌칙을 줌으로써 **일부 가중치를 0으로 만드는 희소성(sparsity)** 을 유도하고, **과적합을 방지**한다.  
- 주로 **라소 회귀(Lasso)** 의 형태로 선형 회귀 문제에서 활용되며, 딥러닝에서도 가중치 희소화가 필요할 때 사용한다.  
- L1, L2 정규화 모두 “가중치 폭발”을 억제하는 공통 목적을 갖지만, L1은 **희소성**을, L2는 **작고 부드러운 가중치 분포**를 더 잘 유도한다.  
- 실제 모델 설계 시, **목표**(희소성 vs. 안정적 수렴 vs. 해석 가능성 등)에 따라 L1, L2, 혹은 둘의 조합을 적절히 활용한다.
