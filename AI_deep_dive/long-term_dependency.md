---
id: long-term_dependency
aliases:
  - long-term_dependency
tags: []
---

# long-term_dependency
### **장기 의존성 문제 (Long-Term Dependency Problem)란?**  
장기 의존성 문제는 **시퀀스 데이터를 처리할 때 초반 입력이 후반 출력에 영향을 주기 어려운 문제**를 의미한다.  

#### **1. 왜 발생할까?**
- RNN(Recurrent Neural Network) 계열의 모델들은 **이전 상태(state)를 다음 상태로 전달하면서 시퀀스를 처리**한다.
- 하지만, **시퀀스가 길어질수록 초반의 정보가 점점 희석**되어, 나중에는 **초반 정보가 손실**된다.
- 즉, **멀리 떨어진 단어들 간의 관계를 학습하기 어려움** → 문맥 이해가 어려워진다.

#### **2. 예시**
다음 문장을 번역한다고 가정하자.  

> "I grew up in France and I speak fluent **French**."

여기서 **French**라는 단어는 **France**와 강한 연관성이 있다.  
하지만, 만약 문장이 매우 길다면, **"France"라는 정보가 RNN의 상태(State)를 계속 거치면서 희석**되어 **French와의 관계를 모델이 제대로 학습하지 못할 가능성**이 커진다.  

이런 이유로 **장기 의존성 문제**가 발생한다.

---

### **CNN과 필터(Filter)와의 관계**  
CNN(Convolutional Neural Network)은 이미지 처리에서 주로 사용되지만, 텍스트 시퀀스에도 적용될 수 있다.  
하지만 **CNN의 필터(Filter)는 국소적(Local) 특성만 학습**하므로, 장기 의존성 문제를 해결하기에는 한계가 있다.

#### **1. CNN의 필터 동작 방식**
- CNN에서는 **고정 크기(예: 3x3, 5x5)의 필터(커널)**를 사용하여 입력 데이터를 스캔한다.
- 이는 **로컬 정보(Local Information)**를 잘 잡아낼 수 있지만, 필터 크기만큼의 정보만 고려할 수 있다.

#### **2. CNN이 장기 의존성 문제를 해결하지 못하는 이유**
- CNN의 필터는 보통 **로컬 윈도우(Local Window)**에서 패턴을 감지하기 때문에,  
  → 긴 문장에서 **먼 거리의 단어들 간의 관계를 직접 학습하기 어렵다**.
- 물론 CNN을 여러 층(Stacked CNN)으로 쌓으면 더 넓은 범위를 볼 수 있지만,  
  → 여전히 **글로벌 문맥(Global Context)까지 포착하는 것은 어려움**이 있다.

---

### **Attention vs. CNN vs. RNN**
| 모델 | 특징 | 장기 의존성 문제 해결 가능? |
|------|------|-------------------|
| RNN / LSTM / GRU | 순차적으로 정보를 전달 | X (긴 시퀀스에서 정보 희석됨) |
| CNN | 지역적(로컬) 특성 학습 | X (멀리 있는 단어 관계 학습 어려움) |
| **Attention** | 모든 토큰을 한 번에 비교 | **O (멀리 떨어진 단어도 쉽게 연결 가능)** |

즉, **Attention**은 **모든 단어(토큰) 간의 관계를 직접 계산**하므로, 장기 의존성 문제를 해결할 수 있다.  
이것이 **Transformer 모델이 RNN이나 CNN을 대체**하게 된 이유이다.
