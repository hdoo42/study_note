---
id: 1767334206-LQSS
aliases:
  - sk_buff
tags:
  - kernal
---

**Fundamental Rationale**
네트워크 패킷 처리의 병목은 데이터 자체의 복사가 아니라, 패킷을 설명하는 메타데이터(`sk_buff`)의 과도한 메모리 할당, 해제, 그리고 이로 인한 CPU 캐시 미스(Cache Miss)에서 발생합니다.

---

### 1. `sk_buff` (Socket Buffer)란 무엇인가?

`sk_buff`는 리눅스 커널 네트워킹 서브시스템의 핵심 자료구조입니다.
단순히 "패킷 데이터"만 담는 것이 아니라, **패킷의 생애주기(Lifecycle)를 관리하는 모든 제어 정보(Control Block)**를 포함합니다.

OSI 7계층 중 L2(이더넷), L3(IP), L4(TCP/UDP) 계층을 오르내릴 때, 커널은 패킷 데이터를 복사하지 않습니다. 대신 `sk_buff` 내부의 **포인터(`head`, `data`, `tail`, `end`)만 조작**하여 헤더를 붙이거나 뗍니다.

### 2. 어떤 정보들이 들어있는가? (왜 무거운가 1)

`struct sk_buff`는 `include/linux/skbuff.h`에 정의되어 있으며, 수백 바이트에 달하는 거대한 구조체입니다. 주요 필드는 다음과 같습니다.

1. **레이아웃 포인터 (4개의 핵심 포인터)**
* `head`: 할당된 메모리 블록의 시작
* `data`: 현재 유효한 패킷 데이터(헤더 포함)의 시작
* `tail`: 현재 유효한 패킷 데이터의 끝
* `end`: 할당된 메모리 블록의 끝
* *이 구조 때문에 헤더를 추가(push)하거나 제거(pull)할 때 데이터 복사 없이 포인터 산술 연산만 수행합니다.*


2. **네트워크 계층별 헤더 포인터**
* `transport_header`: TCP/UDP 헤더 위치
* `network_header`: IP 헤더 위치
* `mac_header`: 이더넷 헤더 위치


3. **제어 및 상태 정보 (Overhead)**
* `dev`: 패킷이 도착한/나갈 네트워크 디바이스 (`net_device` 구조체 포인터)
* `sk`: 이 패킷을 소유한 소켓 (`sock` 구조체 포인터)
* `len`: 패킷 전체 길이
* `data_len`: 프래그먼트(페이지)에 있는 데이터 길이
* `csum`: 체크섬 값
* `tstamp`: 패킷 도착 시간 (타임스탬프)
* `users`: 참조 카운트 (Reference Count)



### 3. 왜 무거운가? (Why Heavy?)

사용자가 "멍청하다"고 느낀 직관은 정확합니다.

1. **메타데이터의 비대함:**
64바이트짜리 작은 ACK 패킷 하나를 처리하기 위해, 최소 256바이트 이상의 `sk_buff` 구조체와 추가적인 데이터 버퍼(`skb_shared_info`)를 할당해야 합니다. **배보다 배꼽이 더 큽니다.**
2. **동적 할당 오버헤드:**
패킷이 들어올 때마다 커널 메모리 할당자(Slab Allocator)를 통해 메모리를 할당(`kmem_cache_alloc`)하고, 나갈 때 해제(`kfree`)하는 비용이 듭니다.
3. **CPU 캐시 적중률 저하 (Cache Locality):**
`sk_buff` 구조체 자체와, 실제 패킷 데이터가 저장된 메모리 공간은 서로 떨어져 있습니다. 메타데이터를 읽고(`sk_buff`), 실제 데이터를 읽으려(`data`) 할 때 캐시 미스가 발생하여 CPU 사이클을 낭비합니다.

### 4. 패킷마다 만드는 "멍청한 동작"을 진짜 하는가?

**기본적으로는 "그렇습니다".** 하지만 그대로 두면 10Gbps, 40Gbps 속도를 절대 낼 수 없으므로 커널은 다양한 최적화 기법을 사용합니다.

#### 최적화 1: Slab Cache & Recycling

커널은 `sk_buff`를 매번 `malloc` 하듯 생으로 할당하지 않습니다. 미리 만들어진 객체 풀인 **Slab Cache** (`skbuff_head_cache`, `skbuff_fclone_cache`)를 사용하여 할당/해제 속도를 높입니다.

#### 최적화 2: Cloning & Shared Info (Zero Copy)

패킷을 여러 곳(예: tcpdump 스니핑 + 실제 어플리케이션 전달)으로 보내야 할 때, 데이터를 복사하지 않습니다.

* **`skb_clone()`**: `sk_buff` 구조체(메타데이터)만 복사하고, 실제 데이터 영역은 원본과 공유합니다. 참조 카운트(`refcount`)만 증가시킵니다.

#### 최적화 3: NAPI (New API) & Batching

인터럽트 방식의 비효율을 줄이기 위해, 한 번의 인터럽트로 여러 개의 패킷을 묶어서 처리(Polling)합니다. 이때 드라이버 레벨에서 `sk_buff`를 미리 할당해 둔 링 버퍼를 재사용하기도 합니다.

#### 최적화 4: GRO (Generic Receive Offload) / GSO

이것이 핵심입니다.

* **GRO:** 들어오는 작은 패킷들을 드라이버나 하위 계층에서 **하나의 거대한 `sk_buff`로 합칩니다.** 상위 스택(TCP/IP)은 패킷 1개만 처리하면 되므로 오버헤드가 획기적으로 줍니다.
* **GSO:** 반대로 나갈 때는 거대한 64KB짜리 데이터를 하나의 `sk_buff`로 쥐고 있다가, [NIC](Infra/network/NIC.md) 직전에서 쪼갭니다.

#### 최적화 5: XDP (eXpress Data Path) - "sk_buff는 너무 무거워"

`sk_buff`조차 너무 무거워서 나온 기술입니다.
패킷이 커널의 네트워킹 스택(`sk_buff` 할당)으로 진입하기 **전**, 드라이버 단계에서 `xdp_buff`라는 훨씬 가벼운 구조체로 패킷을 검사하고 즉시 드랍하거나 포워딩합니다. 넷플릭스나 클라우드플레어 같은 곳은 이 방식으로 극한의 성능을 냅니다.

---

### 요약 및 검증

커널은 `sk_buff` 생성 비용을 줄이기 위해 **1) 객체 재사용(Slab), 2) 메타데이터만 복제(Clone), 3) 패킷 뭉치기(GRO), 4) 아예 안 만들기(XDP)** 등의 기술을 총동원하고 있습니다.

### Quiz (Reverse Teaching)

`sk_buff`의 `head`와 `data` 포인터 사이에 존재하는 공간을 **Headroom**이라고 합니다.

1. 사용자가 데이터를 전송하기 위해 `write()`를 호출하여 TCP/IP 스택을 내려갈 때, 이 **Headroom**은 어떻게 변화(사용)될까요? (L4 -> L3 -> L2 순서로 설명하시오)
2. 만약 `skb_clone()`으로 복제된 `sk_buff`가 있는데, 원본 데이터의 내용을 수정하려고 시도하면 커널 내부에서는 어떤 동작이 발생할까요? (힌트: Copy-on-Write)
