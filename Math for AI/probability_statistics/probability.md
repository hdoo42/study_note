---
id: probability
aliases:
  - 확률
  - 확률론
tags: [probability, statistics, mathematics, AI]
---

# 확률 (Probability)

확률은 불확실성을 수학적으로 표현하는 방법으로, 특히 인공지능과 기계학습 분야에서 핵심적인 개념입니다. 이 문서에서는 확률의 수학적 정의, 인공지능에서 확률의 중요성, 그리고 생성형 AI와 확률의 관계에 대해 설명합니다.

## 확률의 수학적 정의

확률은 수학적으로 **집합에 대해 정의된 함수**입니다. 보다 정확하게, 확률은 '사건(event)'이라는 집합에 실수 값을 대응시키는 함수이며, 이 값은 항상 0 이상 1 이하입니다.

### 기본 요소

- **표본공간** $\Omega$: 가능한 모든 결과들의 집합
- **사건(Event)**: $\Omega$의 부분집합
- **확률함수** $P$: 사건에 대해 실수 $[0, 1]$ 범위의 값을 반환하는 함수

수학적으로 표현하면:

$$
P: 2^{\Omega} \to [0, 1]
$$

### Kolmogorov의 공리계

확률 함수 $P$는 다음의 세 가지 공리를 만족해야 합니다:

1. **비음성(Non-negativity)**  
   모든 사건 $A$에 대해: $P(A) \geq 0$

2. **전체 확률은 1(Normalization)**  
   표본공간 전체의 확률은 1입니다: $P(\Omega) = 1$

3. **가산성(Countable Additivity)**  
   서로 배반(disjoint)인 사건들 $A_1, A_2, \ldots$에 대해: 
   $P\left(\bigcup_{i=1}^{\infty} A_i\right) = \sum_{i=1}^{\infty} P(A_i)$

## 인공지능과 확률

인공지능이 확률을 사용하는 이유는 우리가 살고 있는 세계의 본질적인 불확실성 때문입니다.

### 불확실한 세계에서의 모델링

우리는 **불확실한 세계(uncertain world)** 속에 살고 있습니다. 이 세계에서의 관찰은 항상 **노이즈**, **부분적 정보**, **모호함**을 수반합니다. 인공지능이 이러한 세상을 이해하고 예측하려면, **확률적 모델링**이 필수적입니다.

### 불확실성의 종류

1. **본질적 불확실성**: 세상의 사건 대부분은 **결정론적이지 않음**
2. **인식론적 불확실성**: 데이터의 노이즈, 불완전한 관측
3. **계산적 불확실성**: 모든 가능성을 고려할 수 없는 계산적 한계

### AI 시스템에서의 확률 활용

1. **학습(Learning)**: 기계 학습은 **데이터로부터 패턴을 추정**하는 과정으로, 본질적으로는 조건부 확률 $P(Y|X)$, 즉 "X가 주어졌을 때 Y가 일어날 확률"을 예측합니다.
   - 지도학습에서는 Cross-Entropy와 같은 **확률 기반 손실 함수**를 사용
   - 강화학습에서도 **[[policy_in_rl|정책(policy)]]**은 행동의 확률 분포로 표현됨

2. **추론(Inference)과 의사결정**: 확률을 이용하면 단순한 예측뿐만 아니라 **불확실한 상황에서 최적의 판단**을 할 수 있습니다.
   - 베이즈 정리는 관측된 데이터를 바탕으로 기존의 믿음을 갱신하는 수단
   - 예측 결과를 **확률 값으로 출력**하면, 의사결정 시스템에서 **리스크를 관리**할 수 있음

## 생성형 AI와 확률

생성형 AI(Generative AI)는 **확률 모델**에 깊이 기반한 기술입니다.

### 언어 모델의 확률적 특성

대규모 언어 모델(Large Language Model, LLM)은 다음과 같은 **확률 기반 구조**를 따릅니다:

- 모델은 텍스트 $x$가 주어졌을 때 다음 단어 $y$의 확률 $P(y \mid x)$를 최대화하도록 학습됩니다.
- 또는 문맥 전체에 대한 확률 $P(x, y)$를 높이는 방향으로 작동할 수도 있습니다.
- 이 확률은 방대한 텍스트 코퍼스를 바탕으로 추정되며, 말뭉치의 통계적 경향을 반영합니다.

### LLM의 구조적 특성

LLM은 간단히 다음과 같은 특징을 가진 모델입니다:

- 수십억 개 이상의 파라미터를 가진 **고차원 확률 모델**
- **언어의 통계적 구조를 학습**해 새로운 문장을 생성할 수 있음
- $P(x_1, x_2, \ldots, x_n)$ 또는 $P(x_t \mid x_1, \ldots, x_{t-1})$ 형식의 조건부 확률 구조를 따름

## 관련 개념

- [[law_of_large_numbers|큰 수의 법칙]]: 표본 크기가 충분히 크면 표본 평균이 모집단의 기댓값에 수렴
- [[markov_inequality|마르코프 부등식]]: 확률 변수의 기댓값을 이용한 확률적 상한
- [[chebyshev_inequality|체비셰프 부등식]]: 분산을 이용한 확률적 상한
- [[borel_cantelli_lemma|보렐-칸텔리 보조정리]]: 확률 수렴 특성 분석

---

확률은 인공지능의 근본적인 기반을 형성하는 수학적 도구입니다. 생성형 AI나 기계학습 알고리즘은 모두 확률론적 프레임워크 내에서 작동하며, 불확실한 세계를 모델링하고 이해하는 데 확률적 접근 방식을 활용합니다.