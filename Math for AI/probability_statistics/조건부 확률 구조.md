$P(x_1, x_2, \ldots, x_n)$
$P(x_t \mid x_1, \ldots, x_{t-1})$    

는 **LLM이 언어 데이터를 어떻게 확률적으로 모델링하는지**를 설명하는 것입니다.

---

### 의미 설명:

1. **$P(x_1, x_2, \ldots, x_n)$**  
    → 하나의 문장을 구성하는 모든 단어(또는 토큰) $x_1$부터 $x_n$까지 **전체 문장의 등장 확률**입니다.  
    예: "나는 밥을 먹었다" 라는 문장이 등장할 확률을 통째로 모델링한다는 뜻입니다.
2. **$P(x_t \mid x_1, \ldots, x_{t-1})$**  
    → **다음 단어가 앞 단어들에 조건부로 결정되는 확률**입니다.  
    즉, 이전에 나온 단어들이 있을 때, 그 다음 단어로 무엇이 나올지를 예측하는 방식입니다.    
예:
 - 앞에 "나는 밥을"이 나왔다면
 - 다음 단어 $x_t$로 "먹었다"가 나올 확률을 계산하는 구조입니다.


---

### 왜 이게 중요할까?

이 조건부 확률 구조는 LLM의 **텍스트 생성 원리**의 핵심입니다. 모델은 이전 단어들을 기반으로 다음 단어를 확률적으로 선택하면서 문장을 점점 길게 만들어갑니다. 이것을 **오토리그레시브 모델링 ([[autoregressive modeling]])** 이라고 부릅니다.

---

궁금한 점 더 알려드릴까요?