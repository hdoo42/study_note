
뉴럴 네트워크(Neural Network)에서 모든 **엣지의 가중치($W$)** 와 **세타값(편향, $b$)** 을 초기 상태에서 설정하는 것은 학습의 성공에 매우 중요합니다. 잘못된 초기화는 학습 속도를 늦추거나, 기울기 소실/폭주 문제를 발생시켜 네트워크가 수렴하지 못하게 할 수 있습니다.

가장 일반적이고 효과적인 초기화 방법들에 대해 간단히 설명하겠습니다.

---

### 1. 가중치($W$) 초기화
**가중치**는 보통 **작고 무작위적인 값**으로 초기화해야 합니다.
- **영(Zero) 초기화 방지:** 모든 가중치를 0으로 설정하면, 모든 뉴런이 동일한 출력을 내고 동일한 기울기를 가지게 되어 **대칭 파괴(Symmetry Breaking)** 가 일어나지 않아 학습이 불가능해집니다. 따라서 **무작위성**이 필수입니다.
- **표준적인 무작위 초기화:**
    - **작은 무작위 값:** 평균이 0이고 표준편차가 매우 작은 **가우시안 분포(Gaussian Distribution)** 또는 **균등 분포(Uniform Distribution)** 에서 값을 추출합니다. 예를 들어, $\mathcal{N}(0, 0.01)$ 같은 분포를 사용합니다.
- **He 초기화 (ReLU 활성화 함수에 적합):**
    - **ReLU** 함수를 사용할 때 주로 사용됩니다.        
    - 가중치를 평균이 0이고 분산이 $\frac{2}{n_{in}}$인 정규분포에서 추출합니다. 여기서 $n_{in}$은 입력 뉴런의 개수입니다
$$W \sim \mathcal{N} \left( 0, \sqrt{\frac{2}{n_{in}}} \right)$$
  
- **Xavier (Glorot) 초기화 (Sigmoid, Tanh 활성화 함수에 적합):**    
    - **[[sigmoid]]**나 **Tanh** 함수를 사용할 때 주로 사용됩니다.
    - 가중치를 평균이 0이고 분산이 $\frac{1}{n_{in}}$ 또는 $\frac{2}{n_{in} + n_{out}}$인 분포에서 추출합니다. 여기서 $n_{out}$은 출력 뉴런의 개수입니다.
        
        $$W \sim \mathcal{U} \left( -\sqrt{\frac{6}{n_{in} + n_{out}}}, \sqrt{\frac{6}{n_{in} + n_{out}}} \right)$$
        

**He 초기화**와 **Xavier 초기화**는 레이어를 통과하며 활성화 값의 분산이 유지되도록 설계되어, **기울기 소실/폭주** 문제를 완화하는 데 도움을 줍니다.

---

### 2. 편향($b$, 세타값) 초기화

**편향**은 보통 **0**으로 초기화합니다.

- **0 초기화:** 대부분의 경우, 편향 $b$는 **0**으로 설정하는 것이 일반적이며 안전한 방법입니다. 초기에는 학습의 비대칭성을 가중치에만 의존하게 하여 네트워크가 입력 데이터에 대해 **최소한의 편향**을 갖도록 합니다.
    
- **약간의 양수 초기화 (ReLU):** 드물지만, ReLU 활성화 함수를 사용할 경우 초기 **"Dead ReLU"** 문제를 방지하기 위해 편향을 0.01 같은 **작은 양수**로 초기화하는 경우도 있습니다.
    

---

### 🔑 요약

|**요소**|**일반적인 초기화 방법**|**주요 활성화 함수**|
|---|---|---|
|**가중치($W$)**|**He 초기화** $\left( \mathcal{N}(0, \sqrt{2/n_{in}}) \right)$|ReLU|
|**가중치($W$)**|**Xavier 초기화**|Sigmoid, Tanh|
|**편향($b$)**|**0**|모든 활성화 함수|

이 초기화 값들은 학습을 시작하기 위한 **출발점**일 뿐이며, 네트워크는 경사 하강법을 통해 이 값들을 점진적으로 조정해 나갑니다.

다른 질문이나 특정 네트워크 구조에 대한 초기화 방법이 궁금하신가요?