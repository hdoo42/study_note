## K-Nearest Neighbors (KNN)의 `fit()` 결과물 📊

**KNN**은 **지도 학습(Supervised Learning)** 알고리즘 중 하나이며, 주로 **분류(Classification)**나 **회귀(Regression)** 문제에 사용됩니다.

KNN의 `fit(X, y)` 메서드를 실행했을 때의 결과물과 그 의미는 다음과 같습니다.

- **결과물:** `fit()` 메서드는 **학습된 모델 객체 자체**를 반환하며, 이 객체는 **아무것도 계산하거나 변형하지 않은 원본 훈련 데이터 $(X, y)$** 를 내부적으로 저장합니다.
    
- **의미:**
    
    - **실제 학습이 없음:** KNN은 **게으른 학습(Lazy Learning)** 방식입니다. `fit()` 단계에서는 복잡한 수학적 모델을 구축하거나 가중치를 계산하지 않고, 단순히 주어진 **특징 데이터 $X$**와 **레이블 $y$** 쌍을 메모리에 보관하는 것이 전부입니다.
        
    - **예측 단계에서 계산:** 모든 계산(거리 측정, 최근접 이웃 찾기)은 새로운 데이터 포인트가 들어와 **`predict()`** 메서드가 호출될 때 수행됩니다.
        
    - **즉, KNN의 `fit()`은 나중에 사용할 훈련 데이터셋 자체가 곧 모델입니다.**
        

---

## K-Means Clustering의 `fit()` 결과물 🌀

**K-Means Clustering**은 **비지도 학습(Unsupervised Learning)** 알고리즘 중 하나이며, 주로 **군집화(Clustering)** 문제에 사용됩니다.

K-Means의 `fit(X)` 메서드를 실행했을 때의 결과물과 그 의미는 다음과 같습니다.

- **결과물:** `fit()` 메서드는 **학습된 모델 객체 자체**를 반환하며, 이 객체는 학습 과정에서 결정된 **최종 군집 중심(Cluster Centroids)** 의 위치를 내부적으로 저장합니다.
    
- **의미:**
    
    - **실제 학습이 진행됨:** K-Means는 **탐욕적 학습(Eager Learning)** 방식으로, `fit()` 단계에서 반복적인 최적화 과정을 거쳐 학습을 완료합니다.
        
    - **군집 중심 결정:** 알고리즘은 주어진 데이터 $X$를 지정된 $K$개의 군집으로 나누기 위해, 각 데이터 포인트를 가장 가까운 중심에 할당하고, 그 할당된 포인트들의 평균으로 중심을 업데이트하는 과정을 반복합니다.
        
    - **최종 결과:** `fit()`이 완료된 후, 모델 객체는 **`cluster_centers_`** 속성에 **$K$개의 군집 중심 좌표**를 갖게 되며, 이 중심들이 새로운 데이터 포인트를 어느 군집으로 분류할지 결정하는 데 사용됩니다.
        

---

### **요약 비교**

|**특징**|**K-Nearest Neighbors (fit(X, y))**|**K-Means Clustering (fit(X))**|
|---|---|---|
|**학습 유형**|지도 학습 (Supervised)|비지도 학습 (Unsupervised)|
|**학습 방식**|**게으른 학습 (Lazy Learning)**|**탐욕적 학습 (Eager Learning)**|
|**`fit()` 역할**|**훈련 데이터 $(X, y)$ 저장**|**최종 군집 중심 좌표 계산 및 저장**|
|**모델 결과물**|원본 훈련 데이터셋 자체가 모델|$K$개의 군집 중심 좌표|

두 알고리즘의 학습 방식이 다르기 때문에, 같은 `fit()` 메서드라도 내부적으로 수행하는 작업과 결과로 저장되는 정보가 완전히 다릅니다.

두 알고리즘은 학습 유형(지도 vs. 비지도)과 학습 방식(게으른 vs. 탐욕적)에서는 큰 차이가 있지만, **근본적인 작동 원리**에는 다음과 같은 중요한 공통점과 유사성이 있습니다.



---

### 1. **핵심 매개변수 'K'의 사용**

- **가장 명확한 공통점**은 두 알고리즘 모두 **$K$**라는 핵심 하이퍼파라미터(Hyperparameter)를 사용한다는 것입니다.
    
    - **KNN:** **$K$**는 예측에 사용할 **가장 가까운 이웃의 수 (Number of Neighbors)**를 의미합니다.
        
    - **K-Means:** **$K$**는 데이터를 분할할 **군집의 수 (Number of Clusters)**를 의미합니다.
        
- 두 경우 모두 $K$ 값의 설정이 모델의 성능과 결과 해석에 매우 중요한 영향을 미칩니다.
    

### 2. **거리 측정(Distance Metric) 기반 작동**

- 두 알고리즘은 데이터 포인트 간의 **유사성을 측정**하기 위해 **거리(Distance)** 개념에 전적으로 의존합니다.
    

![Euclidean distance formula 이미지](https://encrypted-tbn1.gstatic.com/licensed-image?q=tbn:ANd9GcQzv8qHHnGjXz4HLCZhpsolXoFkeRE-Tb5FjhXLEkavLDMAcCYRuRIybAKpL9q14y4R4e5pQLl3UGMuKduA6fiKC3vPmd_EIaGTrLyvzme8uZNJsow)

Shutterstock

```
* **KNN:** 새로운 데이터 포인트와 **훈련 데이터 포인트** 사이의 거리를 계산하여 가장 가까운 $K$개의 이웃을 찾습니다. (가장 흔하게 유클리드 거리 사용)
* **K-Means:** 각 데이터 포인트와 **군집 중심(Centroid)** 사이의 거리를 계산하여 가장 가까운 군집에 할당합니다. (대부분 유클리드 거리 사용)
```

- 어떤 거리 측정 기준(예: 유클리드 거리, 맨해튼 거리 등)을 사용하느냐에 따라 결과가 달라질 수 있습니다.
    

### 3. **비모수적(Non-parametric) 또는 단순 가정 모델**

- 두 알고리즘 모두 데이터가 특정 확률 분포(예: 정규 분포)를 따른다는 **복잡한 통계적 가정을 하지 않습니다.**
    
    - **KNN:** 새로운 데이터 포인트의 레이블은 주변 이웃의 레이블에 의해 결정됩니다.
        
    - **K-Means:** 데이터 포인트는 가장 가까운 중심에 의해 할당됩니다.
        
- 이러한 단순성 덕분에 데이터의 복잡한 형태에 비교적 유연하게 적용할 수 있습니다.
    

### 4. **데이터 스케일(Scale) 민감성**

- 두 알고리즘 모두 거리 계산을 기반으로 하기 때문에, 특징(Feature)들의 **척도(Scale)에 매우 민감**합니다.
    
- 만약 한 특징의 값 범위가 다른 특징의 값 범위보다 훨씬 크다면, 그 특징이 거리 계산을 **지배(Dominate)**하게 되어 결과가 왜곡될 수 있습니다.
    
- 따라서, 두 알고리즘을 사용하기 전에는 일반적으로 **데이터 전처리(예: 정규화 또는 표준화)**가 필수적입니다.