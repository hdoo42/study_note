---
id: main
aliases: []
tags: []
---
# What is information

# Quantifying information

수학자들은 특정 상황에 대한 불확실성을 모델링하기 위해 **확률 변수(random variable)** 라는 개념을 도입하는 것을 좋아합니다. 우리가 다룰 응용에서는 항상 **유한한 N개의 선택지**가 존재하는 상황을 다루게 되므로, 우리는 집합  
$\{x_1, x_2, \dots, x_N\}$
에서 N개의 가능한 값 중 하나를 취할 수 있는 **이산 확률 변수(discrete random variable)** $X$를 사용합니다. 확률 변수 $X$가 값 $x_1$을 가질 확률은 $p_1$, $x_2$를 가질 확률은 $p_2$, 이런 식으로 정의됩니다. 확률이 작을수록, $X$가 그 특정 값을 가질 것이라는 불확실성은 더 커집니다.

클로드 섀넌(Claude Shannon)은 정보 이론(theory of information)의 기념비적인 연구에서, $X$가 값 $x_i$를 가졌다는 사실을 알게 되었을 때 우리가 얻게 되는 **정보량(information)** 을 다음과 같이 정의했습니다:

$$
I(x_i) = \log_2 \left( \frac{1}{p_i} \right) \text{ bits}.
$$

여기서 주목할 점은, **어떤 선택의 불확실성은 그 선택의 확률에 반비례한다**는 것입니다. 로그 안에 있는 항은 본질적으로 해당 선택에 대한 불확실성을 나타냅니다. 우리는 **$\log_2$** 를 사용해 불확실성의 크기를 비트(bits) 단위로 측정합니다. 여기서 비트는 0 또는 1의 값을 가질 수 있는 최소 단위입니다.  
정보량이란, 이 선택을 인코딩하기 위해 필요한 비트 수로 생각할 수 있습니다.

